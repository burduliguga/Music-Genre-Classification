# Music-Genre-Classification

Introduction:
Problem is to classify music with it genre. Dataset was GTZAN, which consists 1000 audio file, each with 30 seconds of duration. There are 10 different genre and each genre consists 100 audio file. The genres are: Blues, Classical, Country, Disco, Hiphop, Jazz, Metal, Pop, Reggae and Rock. The idea is to classify different musical genres automatically.

Approach:
First of all, I have to study data: How to work with audio files, get features. As I saw there is a python library librosa, which is for working to music files. With the librosa library I get music file as wave data and sampling_rate. After getting wave data and sampling rate, I used also librosa’s method feature.melspectrogram, which gives melspectogram matrix according to wave data and sampling rate. Additionally, I get the labels from the names of the files, their first parts were categories. With feature.melspectrogram we will get matrixes like this below. If we use music files with full length (30 seconds) it gives melspectrogram with shape (128, 1293).

Firstly I tried to use the whole size of the files, however the dimensions were huge, so I use only first 10 seconds of the music files instead of full 30 second. Another problem was that in the GTZAN dataset there are 1000 examples, 100 examples for each class, which is a few to get good results after training. In that case, as we mentioned in class we can use cross-correlation (split this data for example into 5 pieces and use each piece as test data), however it need more time for training dataset. So, I decide to use another very popular method Data Augmentation. With data augmentation it is possible to generate synthetic data from existing data. I used two different data augmentation methods: Shifting Time and Changing Pitch with 2 different parameters for each method (0.8 and 0.9 as pitch factor and -2 and 2 as time shifting). To compare results, I plotted spectrograms for one blues song. First one is original, second one is after pitching with pitch factor 0.8 and third one is time shifting with shift factor 2. As we see they are very similar and we can use all this information as the data for label blues.
After these operations I get the data with shapes (128, 431) total amount of 3000. I checked that data was balanced in that case as well (get 300 examples for each class). Now it was time for splitting data into Training, Test and Validation data. I split it with proportion 60%-20%-20%. After getting the Training, Test and Validation datasets its time to create model. The labels where strings in my case (‘blues’, ‘classic’, ‘rock’ and etc.), so I created two methods: one for converting the string into numbers
and the second one, which converts number into string. Because I had 10 classes, I had numbers from 0 to 9. After converting label data(Y_test, Y_validation and Y_test) into numbers, I use one-hot encoding to finally get the labels from categorical data.
As I mentioned above I planed to use Convolutional Neural Network (CNN) for training the data. In the model I have 3 convolutional layer, after each layer I am using max_pooling method to reduce shapes and activation function ‘RELU’ . RELU is widely used especially in CNN, it is easy to compute, don’t have gradient vanish problem and is not saturate. After the three layers, I have dropout, which is used for ‘brain damage’, improves robustness, hence reduces overfitting. Final output layer is with “Softmax” activation function, because it is used with multi-class classification. Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. 

This additional constraint helps training converge more quickly than it otherwise would. As we can see, firstly we had shape (124, 427, 32) (I had 32 filters 5x5). After each max pooling it decreased twice (because size of pooling area was 2).
For model compile I tried two different optimizers with two different barch_size: first was sgd and second was adam. I used categorical crossentropy loss function in both cases. However, I get much better results with the adam optimizer. I had learning rate 0.01 and I only used 10 epochs for training, because it needed time.
After 10 epochs, with categorical crossentropy loss function and adam optimizer, I get quite good result. From 300 test examples I get 239 correct answers, which is approximately 80% accuracy. With more training will be better results. 


Conclusion:
To sum up, from the dataset GTZAN, I get the music files with 10 different categories. They were 30 seconds long, I cut only first 10 seconds. Then using python library librosa I get spectrograms from the music files and use it as training data. Before using as training data labels needed converting from categorical to numerical, so I use one-hot encoding and get the numerical labels. The dataset had only 1000 examples, so I use data augmentation for creating artificial data from the original (used two methods: time shifting and pitch changing). After get the input data, I create a model with 3 convolutional layer, with RELU activation function. I had maxpooling to reduce dimensions after
each layer. Final layer had Softmax activation function, because we have multi-class classification problem. After some small training I get satisfactory results, with 80% accuracy.
In comparison to k-NN approach, which I saw in the internet they had accuracy about 70%. However, as I saw they use different information from the files. They get mfcc from the wave data and sample rate. With mean matrix and covariance they get the distances and hence the neighbors for their model.

References:
https://data-flair.training/blogs/python-project-music-genre-classification/
https://www.tensorflow.org/datasets/catalog/gtzan
https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification
https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6
